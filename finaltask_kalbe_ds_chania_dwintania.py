# -*- coding: utf-8 -*-
"""FinalTask_Kalbe_DS_Chania Dwintania.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vowMtDaAdXS-AJJiTna6XLv_B9c0Hop0

##**Kalbe Machine Learning Project**

##Import Library
"""

import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.metrics import mean_absolute_error, mean_squared_error
from sklearn import preprocessing
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.holtwinters import SimpleExpSmoothing, Holt
from statsmodels.tsa.arima.model import ARIMA
from pandas.plotting import autocorrelation_plot

import warnings
warnings.filterwarnings('ignore')

"""Baca Data"""

!gdown --fuzzy https://drive.google.com/file/d/1CC4x56VToictkVWmFESe2_DU3dTsHUkS/view?usp=sharing
!gdown --fuzzy https://drive.google.com/file/d/19ipVAKd0v5AuRbLo602f2iLHDeSeQ1OW/view?usp=sharing
!gdown --fuzzy https://drive.google.com/file/d/1MPrLZpY6Xl6lO6kgsIRykM25EvajvAeR/view?usp=sharing
!gdown --fuzzy https://drive.google.com/file/d/1J7Qmjpc1W0nbKNOEX68foxLlzswFWVU4/view?usp=sharing
cust =pd.read_csv("Case Study - Customer.csv", delimiter=';')
product =pd.read_csv("Case Study - Product.csv",delimiter=';')
store =pd.read_csv("Case Study - Store.csv",delimiter=';')
transc =pd.read_csv("Case Study - Transaction.csv", delimiter=';')

cust.shape, product.shape, store.shape, transc.shape #melihat jumlah baris dan kolom

"""##Data Cleansing"""

cust.head()

cust.duplicated().sum() #tidak ada duplicated value

cust.isna().sum()

cust = cust.dropna()
cust.isna().sum()

cust.shape

product.head()

product.duplicated().sum()

product.isna().sum()

store.head()

store.duplicated().sum()

store.isna().sum()

transc.head()

#data cleansing customer
cust['Income'] = cust['Income'].replace('[,]','.',regex = True).astype('float')

#data cleansing store
store['Latitude'] = store['Latitude'].replace('[,]','.',regex = True).astype('float')
store['Longitude'] = store['Longitude'].replace('[,]','.',regex = True).astype('float')

#data cleansing transaction
transc['Date'] = pd.to_datetime(transc['Date'])
transc = transc.groupby('TransactionID').apply(lambda x: x.loc[x['Date'].idxmax()]) #membuat group berdasarkan transaction id dan mmeilih yang terbaru
transc.reset_index(drop = True, inplace = True) #memastikan index lama sudah dihapus

transc['TransactionID'].value_counts()

transc[ transc['TransactionID'] == 'TR71313']

transc.duplicated().sum()

transc.isna().sum()

"""##Penggabungan Data"""

df_merge = pd.merge(transc, cust, on=['CustomerID'])
df_merge = pd.merge(df_merge, product.drop(columns=['Price']), on=['ProductID'])
df_merge = pd.merge(df_merge, store, on=['StoreID'])

df_merge.head()

df_merge.to_csv('Case Study-Compile.csv', index=False)

"""##**Model Machine Learning Regresi**"""

df_regresi = df_merge.groupby(['Date']).agg({
  'Qty': 'sum'
}).reset_index() #membuat dataframe baru untuk regresi

df_regresi #ada 365 row karena data harian, time series biasa hanya 2 kolom

decomposed = seasonal_decompose(df_regresi.set_index('Date')) #decompose data time series jadi tren, seasonal, resid
plt.figure(figsize=(8,8))

plt.subplot(311)
decomposed.trend.plot(ax=plt.gca())
plt.title('Trend') #memplot komponen tren
#tren cenderung tidak terllau beraturan namun menurun di tengah bulan dan naik tiap di awal dan akhir bulan
plt.subplot(312)
decomposed.seasonal.plot(ax=plt.gca())
plt.title('Seasonality')
#memiliki pattern seasonality
plt.subplot(313)
decomposed.resid.plot(ax=plt.gca())
plt.title('Residuals')

plt.tight_layout()

"""##Check stationarity data

Menggunakan augmented dickey fuller test
"""

from statsmodels.tsa.stattools import adfuller
result = adfuller(df_regresi['Qty'])
print('ADF Statistic: %f' % result[1])
print('Critical Values: ')
for key, value in result[4].items():
  print('\t%s: %.3f' % (key,value))

#adf statistic < critical value --> tolak H0, data time series stasioner -> tidak memiliki tren/ seasonality signifikan, cocok untuk forecasting

cut_off = round(df_regresi.shape[0]*0.8)
df_train = df_regresi[:cut_off]
df_test = df_regresi[cut_off:].reset_index(drop=True)
df_train.shape, df_test.shape

df_train #untuk time series data harus berurutan tanggalnya

df_test

plt.figure(figsize = (20,5))
sns.lineplot(data = df_train, x = df_train['Date'], y = df_train['Qty'])
sns.lineplot(data=df_test, x=df_test['Date'], y = df_test['Qty'])
#tidak ada tren naik, hanya menunjukkan seasonality

autocorrelation_plot(df_regresi['Qty'])
#digunakan untuk mencari p,d, dan q untuk ARIMA

from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
plot_acf(df_regresi['Qty'], lags=40)
plot_pacf(df_regresi['Qty'], lags=40)

import itertools
import pandas as pd
import numpy as np
from statsmodels.tsa.arima.model import ARIMA

p_values = range(0, 50)
d_values = range(0, 50)
q_values = range(0, 50)

best_aic = np.inf
best_order = None

for p, d, q in itertools.product(p_values, d_values, q_values):
    try:
        model = ARIMA(df_regresi, order=(p, d, q))
        model_fit = model.fit()
        aic = model_fit.aic
        if aic < best_aic:
            best_aic = aic
            best_order = (p, d, q)
    except:
        continue

print(f"Best AIC: {best_aic}")
print(f"Best (p, d, q) order: {best_order}")

def rmse(y_actual, y_pred):
    """
    function to calculate RMSE
    """

    print(f'RMSE value {mean_squared_error(y_actual, y_pred)**0.5}')

def eval(y_actual, y_pred):
    """
    functional to eval machine learning modelling
    """

    rmse(y_actual, y_pred)
    print(f'MAE value {mean_absolute_error(y_actual, y_pred)}')

#ARIMA
df_train = df_train.set_index('Date')
df_test = df_test.set_index('Date')

y = df_train['Qty']

ARIMAModel = ARIMA(y, order=(40, 2, 1))
ARIMAModel = ARIMAModel.fit()

y_pred = ARIMAModel.get_forecast(len(df_test))

y_pred_df = y_pred.conf_int()
y_pred_df['predictions'] = ARIMAModel.predict(start=y_pred_df.index[0], end=y_pred_df.index[-1])
y_pred_df = y_pred_df.loc[df_test.index]
y_pred_out = y_pred_df['predictions']

plt.figure(figsize=(20, 5))
plt.plot(df_train['Qty'], label='Training Data')
plt.plot(df_test['Qty'], color='red', label='Test Data')
plt.plot(y_pred_out, color='black', label='ARIMA Predictions')
plt.title('Forecasted Quantity Sold')
plt.legend()
plt.show()

from sklearn.metrics import mean_squared_error, mean_absolute_error

def evaluate_forecast(pred, aktual):

    mse = mean_squared_error(aktual, pred)
    mae = mean_absolute_error(aktual, pred)
    return mse, mae

mse, mae = evaluate_forecast(y_pred_out, df_test['Qty'])

# Print the results
print("Mean Squared Error:", mse)
print("Mean Absolute Error:", mae)

mean = y_pred_df.mean().round()
print(mean)

"""##**Clustering**"""

df_merge.head()

#identifikasi kolom yang redundant/memiliki korelasi tinggi
df_merge.corr()

df_cluster = df_merge.groupby(['CustomerID']).agg({
    'TransactionID' : 'count',
    'Qty' : 'sum',
    'TotalAmount' : 'sum'
}).reset_index()
df_cluster.head()

data_cluster = df_cluster.drop(columns=['CustomerID'])

data_cluster_normalize = preprocessing.normalize(data_cluster)
data_cluster_normalize

sum_squared_distances = []

for num_clusters in range(1, 11):
    # Membuat model K-Means dengan jumlah kluster saat ini
    kmeans_model = KMeans(n_clusters=num_clusters)
    # Melatih model K-Means dengan data dalam df_cluster
    kmeans_model.fit(df_cluster)
    # Menghitung jumlah jarak kuadrat untuk model ini
    sum_squared_distance = kmeans_model.inertia_
    sum_squared_distances.append(sum_squared_distance)

# Plot jumlah jarak kuadrat terhadap jumlah kluster
plt.plot(range(1, 11), sum_squared_distances, marker='o')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('Sum of Squared Distances')
plt.title('Elbow Method: Finding the Optimal Number of Clusters')
plt.show()

# menentukan jumlah clustin
n_clusters = 4

# membuat model KMeans
kmeans = KMeans(n_clusters=n_clusters, random_state=0)
cluster_assignments = kmeans.fit_predict(df_cluster)

# Membuat scatterplot berdasarkan cluster
plt.figure(figsize=(8, 6))
sns.scatterplot(x=df_cluster['Qty'], y=df_cluster['TotalAmount'], s=50, hue=cluster_assignments, palette='viridis')
plt.xlabel('Quantity Bought')
plt.ylabel('Total Transaction Amount')
plt.title('Customer Clustering by Quantity Bought and Total Transaction Amount')
plt.legend(title='Cluster')
plt.show()

df_cluster['cluster_label'] = cluster_assignments

# Mengelompokkan berdasarkan label klaster dan lakukan agregasi
cluster_summary = df_cluster.groupby(['cluster_label']).agg({
    'CustomerID': 'count',
    'TransactionID': 'mean',
    'Qty': 'mean',
    'TotalAmount': 'mean'
})

# Menampilkan statistik ringkasan untuk setiap klaster.
print(cluster_summary)

"""**Cluster 0** : Total quantity pembelian cenderung cukup rendah dengan total transaction amount yang juga cukup rendah.

**Cluster 1**: Pembeli loyal dengan kuantitas dan juga total amount yang tinggi.

**Cluster 2** : Pembeli dengan total kuantitas dan jumlah transaksi cenderung lumayan tinggi.

**Cluster 3** : Pembeli dengan total kuantitas dan jumlah transaksi terendah.
"""